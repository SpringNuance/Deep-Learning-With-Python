{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3833c4d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7516dfffdca5502c6c7361276690662f",
     "grade": false,
     "grade_id": "cell-d3cb4631b6e47374",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h1><center>TensorFlow Input Pipeline</center></h1>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for CS-EJ3311 - Deep Learning with Python</font></center>\n",
    "<center><font size=\"3\">24.10.-11.12.2022</font></center>\n",
    "<center><font size=\"3\">Aalto University & FiTech.io</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb25b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "460539576750b56e5e30e62140f594ad",
     "grade": false,
     "grade_id": "cell-1eb8ad07e11818e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-danger\">\n",
    "   <h3 align=\"center\"> <b>NOTE: This notebook is optional - not graded.</b> </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce32d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "338196a218b1c87395fd693d69ca0b70",
     "grade": false,
     "grade_id": "cell-3d2c78f810f66a68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Those who are familiar with `scikit-learn` Python package, will remember that most machine learning (ML) methods provided by the package have the same usage pattern. First, we load the training data into numpy arrays that store the features and labels of all training data points. These numpy arrays are then fed to `.fit()` function that implements the learning algorithm for a particular ML model. After the model has been trained, we apply it to the new data points, using `.predict()` function, in order to obtain predictions for the labels of new data points. \n",
    "\n",
    "In contrast, deep learning often involve extremely large training data sets which cannot be stored entirely in a numpy array (we would run out of RAM on a desktop computer). Therefore, deep learning methods use sequential access to training data. This approach ties in nicely with the working principle of stochastic gradient descent (SGD). In particular, we divide the dataset into smaller sets or parts called **batches** and only store a single batch in the working memory (e.g. as numpy arrays). The total number of samples present in a single batch is called **batch size**. \n",
    "\n",
    "After loading a new batch of data, we update the ANN parameters (weights and bias) using one iteration of an SGD variant. We repeat this batch-wise optimization until we have read each data point of the dataset. A sequence of batches that together cover the entire dataset is called an **epoch**. Note that the batch size is a hyperparameter of the resulting deep learning method. The choice of hyperparameter is often based on manual tuning (\"trial and error\") to minimize the validation loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34027fa3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cddddb776ef91597e16203605b7146e7",
     "grade": false,
     "grade_id": "cell-f45dee7ea3a770d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In previous notebooks we focused on deep learning models and optimizers. While model architecture and optimization algorithm are determining factors for overall performance, there is one more factor that can create a performance bottleneck: input pipeline. Opening, reading and preprocessing data consume significant amount of time and resources.\n",
    "\n",
    "The need for efficient input pipelines coincide with increased availability of GPU and TPU hardware accelerators. GPU and TPU are optimized for computations with vectors and matrices, but they are not handling data transformation and preprocessing well. Therefore, data preprocessing is often performed on CPU, which limits efficient use of GPU and TPU. Inefficient use of accelerators in its turn increases financial burden as cost of GPU and TPU use is high, not mentioning an ecological footprint from training large deep neural networks.\n",
    "\n",
    "Let's consider following data pipeline:\n",
    "\n",
    "- Opening a data file if it hasn't been opened yet\n",
    "- Fetching a data entry from the file\n",
    "- Using the data for training\n",
    "\n",
    "In this scenario, the process is sequential: model is sitting idle, while data is read and fetching is stall during training:\n",
    "\n",
    "<img src=\"../../../coursedata/Regularization/naive.png\">\n",
    "\n",
    "**tf.data** is a framework for building and executing efficient input pipelines for deep learning. The idea of tf.data input pipeline is to decouple data delivery and data consumption steps, thus decreasing time spent in idle state. This is achieved with introducing prefetching of data: while model is training on the current samples, the input pipeline prepares data samples for the next training step. This result in overlapping preprocessing with model training computations. \n",
    "\n",
    "<img src=\"../../../coursedata/Regularization/prefetch.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a9a04-7c60-479c-9d5e-bc95004ae2db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9d220617e3188783077ebd2512e0871",
     "grade": false,
     "grade_id": "cell-3dd18320baefcf6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Tensorflow documentation](https://www.tensorflow.org/guide/data_performance#prefetching) states: \n",
    "\n",
    "<div class=\"blockquote-container\">\n",
    "    <blockquote class=\"ludwig\">\n",
    "        The tf.data API provides the tf.data.Dataset.prefetch transformation. It can \n",
    "        be used to decouple the time when data is produced from the time when data is \n",
    "        consumed. In particular, the transformation uses a background thread and an \n",
    "        internal buffer to prefetch elements from the input dataset ahead of the time \n",
    "        they are requested. The number of elements to prefetch should be equal to (or \n",
    "        possibly greater than) the number of batches consumed by a single training \n",
    "        step. You could either manually tune this value, or set it to \n",
    "        tf.data.AUTOTUNE, which will prompt the tf.data runtime to tune the value \n",
    "        dynamically at runtime.\n",
    "    </blockquote>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a39f9-4cf7-40d5-93f1-361cd7e5d234",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce481c2ad433501a5df64fab15d903f6",
     "grade": false,
     "grade_id": "cell-b21070e60678f491",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**tf.data.AUTOTUNE** allows automatically tune parameters such as the degree of parallelism and data prefetch buffer sizes, which are critical for performance, yet often challenging for an average ML user to tune by hand.\n",
    "\n",
    "tf.data API also provides solution for [efficient data extraction](https://www.tensorflow.org/guide/data_performance#parallelizing_data_extraction) from remote storage and for [parallelizing data transformation](https://www.tensorflow.org/guide/data_performance#parallelizing_data_transformation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6dd776",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfd06fa380808faa4df4108a8563c94c",
     "grade": false,
     "grade_id": "cell-4771376c30ec9343",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "- understand the motivation behind building input pipelines. \n",
    "- understand the basic idea of tf.data API. \n",
    "- be able to build simple tf.data.Dataset and apply various transformations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c6623",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6d70474eb1e7b18849bc2081b75dac7",
     "grade": false,
     "grade_id": "cell-538b0729efc2a986",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Additional Material\n",
    "\n",
    "-   tensorflow guide: [\"tf.data: Build TensorFlow input pipelines\"](https://www.tensorflow.org/guide/data#randomly_shuffling_input_data)\n",
    "- tensorflow guide: [\"Better performance with the tf.data API\"](https://www.tensorflow.org/guide/data_performance#parallelizing_data_transformation)\n",
    "- original paper [tf.data: A Machine Learning Data Processing Framework](https://arxiv.org/pdf/2101.12127.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fa627",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b081b2d8c61ebc7ed3ccc160f6881827",
     "grade": false,
     "grade_id": "cell-1faa9a0dababc579",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "tf.data API is based on ETL principle - **Extract, Transform, Load**.\\\n",
    "Data can be of different format: csv & txt files, numpy arrays, images. Various transformation can be applied to tf.data.Dataset, see list [here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#methods_2).\n",
    "\n",
    "<img src=\"../../../coursedata/Regularization/tf.data.png\"/>\n",
    "<center>\n",
    "    <a href=\"https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data\">modified from</a>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19d3dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f473c04f238e330b53321ed23c01249",
     "grade": false,
     "grade_id": "cell-03bcb0487eb7d4c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is a video where creators discuss motivation and basic ideas of tf.data pipelines:\n",
    "https://www.youtube.com/watch?v=VeV65oVvoes\n",
    "\n",
    "tf.data API introduces a tf.data.Dataset abstraction for users to define their input pipeline. Internally tf.data represents an input pipeline dataset as a graph. Let's create our first tf.data.Dataset object from list of numbers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e74e40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aedb98b7aeb16f0b854d99f8d7c8b703",
     "grade": false,
     "grade_id": "cell-00b36dc20064f0ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    <h3><b>Demo.</b>  tf.data.Dataset object from list.</h3>\n",
    "        \n",
    "\n",
    "First step is to extract data. We use `tf.data.Dataset.from_tensor_slices()` expression for lists and tensors to create an instance of tf.data.Dataset object:    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb64c15",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "359fd236635c96e60a4e3073c31be697",
     "grade": false,
     "grade_id": "cell-b3fc49808af5a742",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f71b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e91372a351b5d01bf93f957502eff20",
     "grade": false,
     "grade_id": "cell-b78978b53d3debee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can use dataset as an iterator and iterate samples in for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be064cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fe60a4ccb0b612e20da520e1eaf065e",
     "grade": false,
     "grade_id": "cell-70daafbfaadf8aa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for value in dataset:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f374508",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "daf5ec310c4a0c2a3fcce0dc10b02433",
     "grade": false,
     "grade_id": "cell-c09473e2bf3e7ea8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, the returned values are tensorflow tensors. We can easily convert them into numpy object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e48b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0913a5d1873da289af49ad693e4e3320",
     "grade": false,
     "grade_id": "cell-8ad24195afb91cbd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for value in dataset:\n",
    "    print(value.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a56a01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4dae16be77005c6d557ca69c4e3cca5",
     "grade": false,
     "grade_id": "cell-f78f14935124ad16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can apply various transformations to each sample by chaining functions provided by tf.data. For example:\n",
    "\n",
    "- repeat dataset n-times\n",
    "- apply custom function, e.g. which returns square of a value\n",
    "- shuffle samples \n",
    "- create mini-batches\n",
    "- prefetch samples\n",
    "\n",
    "\"New\" dataset can be generated from the original by repeating samples with `.repeat()` transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ea7ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0322880b73a4bfe35bbefd72b0ade2da",
     "grade": false,
     "grade_id": "cell-fc2f9d68b0d8dabe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.repeat(2) # repeat tensor 2 times\n",
    "\n",
    "for value in dataset:\n",
    "    print(value.numpy()) # print values as type numpy.int32 and not tf.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef0b9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c91f3b2210b67d5b7b19a8d1f27de733",
     "grade": false,
     "grade_id": "cell-ae7a14c72a362a21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To apply custom preprocessing function we write usual Python function and pass this function as an argument to `dataset.map(map_func)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcd072",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ad6eb7325e69c5de88d779041aead15",
     "grade": false,
     "grade_id": "cell-0949cd808a0bc87d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# user defined function\n",
    "def preprocess(x):\n",
    "    return x*x\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.map(preprocess) # apply custom function `preprocess` to data\n",
    "\n",
    "for value in dataset:\n",
    "    print(value.numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f58607",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "636cd4f2bd0a67064c4ee6ab0522097b",
     "grade": false,
     "grade_id": "cell-a987258caab95c57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As input samples are independent, tf.data.Dataset.map transformation allows to parallelize it across multiple CPU cores. The map transformation provides the `num_parallel_calls` argument to specify the level of parallelism.\n",
    "\n",
    "[Tensorflow guide](https://www.tensorflow.org/guide/data_performance#parallelizing_data_transformation): \n",
    "\n",
    "<div class=\"blockquote-container\">\n",
    "    <blockquote class=\"ludwig\">\n",
    "Choosing the best value for the num_parallel_calls argument depends on hardware, characteristics of your training data (such as its size and shape), the cost of your map function, and what other processing is happening on the CPU at the same time. A simple heuristic is to use the number of available CPU cores. However, as for the prefetch and interleave transformation, the map transformation supports tf.data.AUTOTUNE which will delegate the decision about what level of parallelism to use to the tf.data runtime.\n",
    "    </blockquote>\n",
    "</div>\n",
    "\n",
    "Although in our simple example the effect of parallelism is negligible, in general it is advisable to set argument `num_parallel_calls=AUTOTUNE` for `.map()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946992c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fcfb85577bb8d45928110b4c6521ca3",
     "grade": false,
     "grade_id": "cell-461e0b1fe98a11b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set AUTOTUNE\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=AUTOTUNE) # number of parallel calls will be set by AUTOTUNE\n",
    "\n",
    "for value in dataset:\n",
    "    print(value.numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db3582",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddafd5828dfc7883e4f8b81f17e32fcc",
     "grade": false,
     "grade_id": "cell-0563551ec891b4d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To shuffle samples we call `dataset.shuffle(buffer_size)`. The `dataset.shuffle()` transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer. See [here](https://stackoverflow.com/questions/53514495/what-does-batch-repeat-and-shuffle-do-with-tensorflow-dataset) explained example of buffer size for shuffle function. In short, buffer size will affect randomness of a batch, thus it should be at least few times larger than batch size.\n",
    "\n",
    "\n",
    "Batches are formed with `dataset.batch(batch_size)`. As was discussed previously batch size should not exceed memory limits of CPU used.\n",
    "\n",
    "Finally, we can prefetch samples with `dataset.prefetch(buffer_size)`. From [tensorflow docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch): \n",
    "\n",
    "<div class=\"blockquote-container\">\n",
    "    <blockquote class=\"ludwig\">\n",
    "Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
    "    </blockquote>   \n",
    "</div>\n",
    "   \n",
    "        \n",
    "Here, buffer size indicates number of elements to prefetch. For example, `dataset.batch(20).prefetch(2)` will prefetch 2 elements (2 batches, of 20 examples each). Similarly to `.map()` transformation, we can set buffer size to AUTOTUNE in `prefetch()` for automatic estimation of buffer size.\n",
    "\n",
    "You can read discussion about difference of buffer size parameter for these functions [here](https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle).\n",
    "\n",
    "Let's chain all transformations and call `dataset.take()` to create batches of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a21e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f023cf1d4e04e3cf412f7057060aeabb",
     "grade": false,
     "grade_id": "cell-09af2595423270c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "\n",
    "dataset = dataset.repeat(100)\n",
    "dataset = dataset.map(preprocess)\n",
    "dataset = dataset.shuffle(buffer_size=20, seed=42)\n",
    "dataset = dataset.batch(batch_size=5)\n",
    "dataset = dataset.prefetch(buffer_size=2)\n",
    "\n",
    "for count_batch in dataset.take(3):\n",
    "    print(count_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb971a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3ba7e6b64d1c86610975a62380137b3",
     "grade": false,
     "grade_id": "cell-af77cf51f8e4fddc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "or, shorter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d32c06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da7f120a6dc2630712ad538b5ef8a645",
     "grade": false,
     "grade_id": "cell-471bdfeb53e20d1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "\n",
    "dataset = dataset.repeat(100).map(preprocess).shuffle(20, seed=42).batch(5).prefetch(2)\n",
    "\n",
    "\n",
    "for count_batch in dataset.take(3):\n",
    "    print(count_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d565d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88ba0edc12ead518b73a5dc166bbf783",
     "grade": false,
     "grade_id": "cell-b0b358a3c8dabb71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-success\">\n",
    "       <center> <h3>Python Generators.</h3></center>\n",
    "</div>   \n",
    "\n",
    "In this notebook we discuss data generators for deep learning. What are the Python generators in general? Python generator is a function which behaves like an iterator, i.e. it can be used in a for-loop. It looks like a normal function except that it contains `yield` expression (instead of `return`) for producing a series of values usable in a for-loop or that can be retrieved one at a time with the `next()` function link. \n",
    "Look at the example of a simple Python generator function, which generates infinite sequence of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a9b7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85279a28dc9c5b1e9d02d4178391cdc6",
     "grade": false,
     "grade_id": "cell-60ac591e0d28ca70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# define a Python generator function, which can create an infinite sequence of numbers\n",
    "def sequence_generator():   \n",
    "    number = 0\n",
    "    while True:\n",
    "        # `yield` statement pauses the function, saves all its states\n",
    "        # and later continues from there on successive calls\n",
    "        yield number\n",
    "        number += 1\n",
    "\n",
    "numbers = []\n",
    "for number in sequence_generator():\n",
    "    numbers.append(number)\n",
    "    if number>9: # include `break` statement, otherwise the for-loop will iterate infinitely\n",
    "        break\n",
    "        \n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650de30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8a6f2e0dfb59a3e1a91b99674d84512",
     "grade": false,
     "grade_id": "cell-4341a9c03ef3aebc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can access generated values of Python generator by using `next()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45931cdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99498db2c34fcd7af513dfb310e918da",
     "grade": false,
     "grade_id": "cell-5ec5b1729a0bdea0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create generator object\n",
    "a = sequence_generator()\n",
    "# display the type of an object \n",
    "print(\"The variable `a` is \", type(a))\n",
    "\n",
    "# display values generated by generator object by using function `next()`\n",
    "next(a), next(a), next(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00d765",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c0234bda37b83f39f9ff49e81310cfd",
     "grade": false,
     "grade_id": "cell-c82bc78d12667803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-success\">\n",
    "    <h3><b>STUDENT TASK (optional). </b>   tf.data.Dataset object from Python generator.</h3>\n",
    "        \n",
    "In this task you will build tf.data.Dataset object from Python generator.\\\n",
    "Use `tf.data.Dataset.from_generator()` constructor to convert the python generator to a fully functional tf.data.Dataset. Pass python generator `sequence_generator()` to the constructor and set `output_types` argument to tf.int32. See examples in [tensorflow tutorial](https://www.tensorflow.org/guide/data#consuming_python_generators).\n",
    "    \n",
    "After creating tf.data.Dataset object apply following transformations:\n",
    "    \n",
    "- use `preprocess` function to square all generator values. Set `num_parallel_calls` arg to `AUTOTUNE`.\n",
    "- shuffle samples. Set `buffer_size` to 20 and `seed` arg to 42.\n",
    "- create batches of size 5\n",
    "- prefetch batches. Set `buffer_size` arg to `AUTOTUNE`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78558812",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10f2e089715752f400b665b4bd64eec2",
     "grade": false,
     "grade_id": "cell-a4d95ca49fc79993",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# create tf.data.Dataset from python generator:\n",
    "\n",
    "# ds_numbers = ...\n",
    "\n",
    "# transformations:\n",
    "\n",
    "# ds_numbers = ...  # apply preprocess function\n",
    "# ds_numbers = ...  # shuffle\n",
    "# ds_numbers = ...  # batch\n",
    "# ds_numbers = ...  # prefetch\n",
    "\n",
    "# retrieve first batch with as_numpy_iterator() & next() functions\n",
    "first_batch = list(ds_numbers.as_numpy_iterator().next())\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be94bce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de45dfc901e76df16a69e4bfbd8218db",
     "grade": false,
     "grade_id": "cell-ad9e91f69f068a0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# retrieve batches with a for-loop\n",
    "\n",
    "i=1\n",
    "for count_batch in ds_numbers.take(3):\n",
    "    print(\"batch\", i, count_batch.numpy())\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68993ce5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c384735bb5fb617d056f7500bd2b315c",
     "grade": false,
     "grade_id": "cell-3a9e45c0b054342e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "assert len(first_batch)==5, \"Batch size should be 5!\"\n",
    "assert isinstance(first_batch[0], np.int32), \"Set output_types to tf.int32!\"\n",
    "assert first_batch[0]==25, \"First value of `fisrt_batch` list should be 25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5c22a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b3ea435ccac7d3d0569e0f2c5ae072e",
     "grade": true,
     "grade_id": "cell-4f3fd7806f2e6fa8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this is hidden cell \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211607c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7fc6dd9774a20c24edffd9ccddb8d9c",
     "grade": false,
     "grade_id": "cell-9626f5ecb365b6c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    <h3><b>Demo.</b>tf.data.Dataset object from text file.</h3>  \n",
    "    \n",
    "Below we create generator for text data. \n",
    "\n",
    "We use `tf.data.TextLineDataset(filenames)` function that creates a Dataset consisting of lines from one or more text files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2311234",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3891fd072b197f171bd4abf82fb2eb5",
     "grade": false,
     "grade_id": "cell-0a0dd8911850075d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "file_path = '../../../coursedata/Regularization/poem.txt'\n",
    "# create dataset from txt file\n",
    "dataset = tf.data.TextLineDataset(file_path)\n",
    "\n",
    "# print samples from dataset\n",
    "for line in dataset.take(6):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405ae5a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb4e280bf18bd1ae92883ecbcc5c92e5",
     "grade": false,
     "grade_id": "cell-5368b835af28fddb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Similar to numbers and tensors, we can shuffle samples (text lines in this case) and form batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc0ab9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "920d65dece53ad152f77234974bbc927",
     "grade": false,
     "grade_id": "cell-334f047c1ea2ff6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(file_path)\n",
    "\n",
    "for line in dataset.shuffle(20).batch(5).take(2):\n",
    "    print(\"\\nbatch:\\n\", line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b8807",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b7d7edde145cea163219d0207d510f5",
     "grade": false,
     "grade_id": "cell-4098f5b20071bd9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    <h3><b>Demo. </b>Generators with tf.data.Dataset.</h3>\n",
    "        \n",
    "In this demo, we build data generator with tf.data API for a subset of [\"Cats vs Dogs\"](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) dataset. This dataset is in the form of images that belong to two different classes \"cats\" and \"dogs\". Binary label is assigned to each image and can take values either 1 for cat image or 0 for dog image. \n",
    "\n",
    "The images (data points) are stored in the \"cats_and_dogs_small\" directory (in coursedata folder). The main directory contains three subdirectories: 'train', 'test', and 'validation'. These three directories contain, respectively, the training, validation, and test images. The images in these three directories are stored in two subdirectories named \"cats\" and \"dogs\", which contain images belonging to corresponding class labels.\n",
    "     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2928d47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ae80b8e698b65f8d720f7e6ed0ce878",
     "grade": false,
     "grade_id": "cell-9b8fdc33ed6fe0f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os    # library to interact with operating system\n",
    "import pathlib    # library to perform filesystem interactions\n",
    "import matplotlib.pyplot as plt    # library for generating plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897220b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dee1a6d28cccde3b09b848f245a4c126",
     "grade": false,
     "grade_id": "cell-71a4a230e9fa61c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-success\">\n",
    "       <center> <h3>Handling paths with pathlib library.</h3></center>\n",
    "</div>   \n",
    "\n",
    "Images are stored in 'cats_and_dogs_small' folder and the path to it is '../../../coursedata/cats_and_dogs_small'. Instead of defining path as a string, we will use Python pathlib library to define path to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ecdef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae0092c8e48305b124770bb481293112",
     "grade": false,
     "grade_id": "cell-e1b40245282a6fc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The path to the dataset\n",
    "base_dir = pathlib.Path.cwd() / '..' / '..' /  '..' / 'coursedata' / 'cats_and_dogs_small'\n",
    "base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800398fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "298129187fe525b1523826beb936e80c",
     "grade": false,
     "grade_id": "cell-23e9f52508ae8bf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`pathlib.Path.cwd()` function gives a reference to the current directory (the directory in which this notebook is stored): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd50434",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcc0ba4ef048777eef61d5d14bd24419",
     "grade": false,
     "grade_id": "cell-00e4e4a4223a3543",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ab512",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d35bedbea0bd5475c23449b2b1676f8",
     "grade": false,
     "grade_id": "cell-d1ad5c81f89303ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using the operator `/` we concatenate the rest of the path to the collection. Notice that `/` is an operator and not a string. It is a convenient syntax to concatenate parts of a path. But, why cannot we just use a string containing the actual path? The answer is portability. The Jupyter Hub may be running in a Linux environment, but your laptop may be running Windows. If you want to use the same code in both operating systems, then you must use the operator `/` to construct the paths. \n",
    "\n",
    "Notice, that `base_dir` is not a string - it is a PosixPath object. PosixPath object has plenty of attributes and methods ( you can check full list with `dir(base_dir)` or `help(base_dir)`). For example, method  `.glob()` is used to list all files (or sub-directories) in a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1250cc2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf416553d551ecb8ba536ed85c12aa1b",
     "grade": false,
     "grade_id": "cell-46e854f0cdf15337",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(type(base_dir.glob('*')))\n",
    "\n",
    "for file in base_dir.glob('*'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a2071",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d84d2b4ce4ed4c6bea49ebf5f3b7354",
     "grade": false,
     "grade_id": "cell-5627e6dd82e5c1fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can see, that it returns a generator object, that can be used to list files or subdirectories. Input argument of `glob()` is called pattern, and used to select files of a certain type. For example, pattern '$*$' returns all files and '$*$.txt' will return only text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ff602",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b2bfc71e73c8b5b163c283cb9354c54",
     "grade": false,
     "grade_id": "cell-72378a1fcda65e89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "temp_dir = pathlib.Path.cwd()\n",
    "\n",
    "# select only jupyter notebooks\n",
    "for file in temp_dir.glob('*.ipynb'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f7bb0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf26d05a4efed77157281e922bc4163e",
     "grade": false,
     "grade_id": "cell-1c1095f52cc1c380",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can count how many images there are in total by using `glob()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c73dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d28d751a9d9f009c4541c41a26f70f6d",
     "grade": false,
     "grade_id": "cell-3175ff9d69b46fb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# count jpg files in all subdirectories of data_dir\n",
    "image_count = len(list(base_dir.glob('*/*/*.jpg')))\n",
    "print(f'Total number of images in the dataset: {image_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57c2ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1470322e1dfa5e76ac9bc35c8cf844f6",
     "grade": false,
     "grade_id": "cell-766ae4dfd68d09d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can create the datasets. We use the function [tf.data.Dataset.list_files](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files). This function expects a `file_pattern` which can be a string or a list of strings representing the filename wildcard. \"Cats and Dogs\" dataset consist of images, which we can retrieve with `*.jpg` wildcard. For example, to get paths of images in train folder we use `train/*/*.jpg`. Notice, that with `*` symbol after `train/`  we indicate, that JPG images from all subdirectories of `train/` should be listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5295f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a81ea8ca5bb2d1a77d9081043f90c48c",
     "grade": false,
     "grade_id": "cell-0bbe385a869e9ab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create tf.data.Dataset objects from training, validation and test images\n",
    "# use .list_files(file_pattern) function to select files that end with .jpg in each directory\n",
    "\n",
    "train_ds = tf.data.Dataset.list_files(str(base_dir/'train/*/*.jpg'), shuffle=False)\n",
    "val_ds   = tf.data.Dataset.list_files(str(base_dir/'validation/*/*.jpg'), shuffle=False)\n",
    "test_ds  = tf.data.Dataset.list_files(str(base_dir/'test/*/*.jpg'), shuffle=False)\n",
    "\n",
    "for file in train_ds.take(5):\n",
    "    print(file.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb188b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cffa5693ad37a88cbaaf05e498640176",
     "grade": false,
     "grade_id": "cell-49e0fac333e08460",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'Number of images in the training set:\\t {tf.data.experimental.cardinality(train_ds).numpy()}')\n",
    "print(f'Number of images in the validation set:\\t {tf.data.experimental.cardinality(val_ds).numpy()}')\n",
    "print(f'Number of images in the test set:\\t {tf.data.experimental.cardinality(test_ds).numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6cdca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b57b4995abf3fc9058e20cd2cd5ad05",
     "grade": false,
     "grade_id": "cell-af5f2c33a0e0a20f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we have created `tf.data.Dataset` which \"contains\" path for images. In order to actually load images and transform to tensorflow arrays, we need to write custom image loader.\\\n",
    "We use `tf.io.read_file` function to read file and `tf.io.decode_jpeg`to decode a JPEG-encoded image to a uint8 tensor.\\\n",
    "We resize image with `tf.image.resize` which results in resized image encoded as float32 tensor. Finally, we scale tensor to get values in range [0,1]. \n",
    "\n",
    "In addition, we need to assign a label to each loaded and transformed image. We can get class names (\"cats\" or \"dogs\") from file path. For example, we can split path, e.g. `/coursedata/cats_and_dogs_small/train/cats/cat.101.jpg`, by path separator `/` and only select part `cats`, which is a class name.\\\n",
    "To split path we use `tf.strings.split(image_path, os.path.sep)`, which will return array of type:\n",
    "\n",
    "```python\n",
    "<tf.Tensor: shape=(15,), dtype=string, numpy=\n",
    "array([b'', b'Users', b'shamsi', b'Desktop', b'dlpython', b'source', b'Round4', b'..', b'..', b'..', b'coursedata', b'cats_and_dogs_small', b'train', b'cats', b'cat.0.jpg'], dtype=object)>\n",
    "```\n",
    "Class name is at second to last position and we can select it with index [-2]. For example, `tf.strings.split(temp, os.path.sep)[-2]` will result in:\n",
    "\n",
    "```python\n",
    "<tf.Tensor: shape=(), dtype=string, numpy=b'cats'>\n",
    "```\n",
    "\n",
    "We can compare retrieved class name with list of class names `tf.strings.split(temp, os.path.sep)[-2] == ['cats', 'dogs']` and get boolean array:\n",
    "\n",
    "```python\n",
    "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>\n",
    "```\n",
    "\n",
    "Finally, image label can be transformed to integer with `tf.argmax([True, False])`:\n",
    "\n",
    "```python\n",
    "<tf.Tensor: shape=(), dtype=int64, numpy=0>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b961ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04342fc73b8d566d4894b42b7abdcf07",
     "grade": false,
     "grade_id": "cell-1ed4b95a522a712e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['cats', 'dogs']\n",
    "IMG_SIZE = 150\n",
    "\n",
    "def load_image(image_path):\n",
    "    # load image\n",
    "    image = tf.io.read_file(image_path)    # read the image from disk\n",
    "    image = tf.io.decode_jpeg(image, channels=3)    # decode jpeg  \n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])    # resize\n",
    "    image = (image / 255)    # scale \n",
    "    \n",
    "    # get lable value from path\n",
    "    parts = tf.strings.split(image_path, os.path.sep)    # parse the class label from the file path\n",
    "    one_hot = parts[-2] == CLASS_NAMES    # select only part with class name and create boolean array\n",
    "    label = tf.argmax(one_hot)    # get label as integer from boolean array\n",
    "    \n",
    "    return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7fc8d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e7b973b2a3b8fa1642c61e51dbec257",
     "grade": false,
     "grade_id": "cell-30e1574c7479447e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To  apply the function `load_image` to each file in the datasets, we use the function [tf.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map). It's first argument `map_func` defines the transformation applied to each element in the dataset. The `map` function returns a new tf.Dataset containing the transformed elements, in the same order as they appeared in the input. The second parameter `num_parallel_calls` represents the number of elements to process asynchronously in parallel.  If the value `tf.data.AUTOTUNE` is used, then the number of parallel calls is set dynamically based on the available CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064c1a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5db6815c9b51757e25cc47bced4586c",
     "grade": false,
     "grade_id": "cell-d882af16ad6dc39b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "train_ds = train_ds.map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(load_image, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb12b16",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af9c3ea78df882a6a93cac6124c5a0ee",
     "grade": false,
     "grade_id": "cell-702b635585d438a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You may be surprised that the execution of the cell is almost instantaneously. This is because the `map` function is a \"lazy\" transformation, which means that the processing only occurs when you try to consume the dataset. Our final step for creating the tf.Dataset is to configure them to performance:\n",
    "\n",
    "* `shuffle()`: Randomly shuffles the elements of this dataset.\n",
    "* `batch()`: Combines consecutive elements of this dataset into batches.\n",
    "* `prefetch()`: Creates a Dataset that prefetches elements from this dataset. This allows later elements to be prepared while the current element is being processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19a70d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fb453858cb0c57b7287c69bb7cc47a4",
     "grade": false,
     "grade_id": "cell-4234d4a047d2c520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def configure_for_performance(ds):\n",
    "    ds = ds.shuffle(buffer_size=2000)\n",
    "    ds = ds.batch(batch_size=32)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "val_ds   = configure_for_performance(val_ds)\n",
    "test_ds  = configure_for_performance(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7537959",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e758a6c771e8dbbd1f5346105b9b8e9c",
     "grade": false,
     "grade_id": "cell-bba60ae5071f0084",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can retrieve one batch (features and labels) and display images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81fc2a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab9d6c5aadaa405568611bef824f13a2",
     "grade": false,
     "grade_id": "cell-373eb88cd6b6a24a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = train_ds.as_numpy_iterator().next()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image_batch[i])\n",
    "    label = label_batch[i]\n",
    "    plt.title(CLASS_NAMES[label])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcd8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
